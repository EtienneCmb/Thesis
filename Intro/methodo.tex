% #############################################################################
%                             METHODOLOGIE
% #############################################################################
\chapter{Méthodologie}

Cette partie méthodologique sera divisée en deux grandes sous parties visant à présenter :
\begin{enumerate}
	\item L'extraction des features: présentation des méthodes utilisées dans le cadre de l'extraction d'attributs issus de l'activité neuronale. De manière générale, nous avons étudiés des attributs spectraux comprenant:
	\begin{itemize}
		\item Phase et puissance spectrale
		\item Attributs de couplage
	\end{itemize}
	\item Le machine learning: présentation des principaux algorithmes testées dans le cadre du décodage de l'activité neuronale 
\end{enumerate}


\section{Extraction des features}
% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
%                                   FEATURES
% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
Comme nous l'avons décrit précédemment, l'objectif du décodage de l'activité neuronale est d'arriver à extraire des signaux cérébraux une information suffisamment pertinente pour pouvoir discriminer différents types de classes (exemple: mouvement vers la gauche Vs droite). \\
Tout les attributs testés dans le cadre de cette thèse sont des attributs spectraux, donc issus de bandes de fréquences. La plupart de ces outils partagent donc une partie méthodologique commune à savoir, le filtrage. De plus, la plupart sont extraits en utilisant la transformée d'Hilbert. Pour éviter une redondance à travers les attributs, nous allons tout d'abord introduire quelques pré-requis.

\subsection{Pré-requis}
% ********************************************
%               PRE-REQUIS
% ********************************************
\subsubsection{Filtrage}
L'intégralité des filtrages dans cette thèse ont été effectués avec la fonction \textit{eegfilt} (qui a ensuite été reproduite pour le passage à python). De plus, afin d'éviter tout phénomène de déphasage, le fonction \textit{filtfilt} a été systématiquement utilisée afin que le filtre soit appliqué dans les deux sens. Si cette dernière fonctionnalité n'est pas forcément indispensable dans le cadre d'un calcul de puissance, elle est absolument nécessaire pour un calcul de \pacFR.  \\
L'ordre du filtre présenté au dessus dépend de la fréquence de filtrage. Il a systématiquement été calculé en utilisant la méthode décrite par \cite{bahramisharif_propagating_2013}:
\begin{equation}
FiltOrder = N_{cycle} \times f_{s}/f_{oi}
\end{equation} \\
où $f_{s}$ est la fréquence d'échantillonnage, $f_{oi}$ est la fréquence d'intérêt et $N_{cycle}$ est un nombre de cycles définit par $N_{cycle}=3$ pour les oscillations lentes et $N_{cycle}=6$ pour les oscillations rapides. 

\subsubsection{Transformée d'Hilbert}
Transformée permettant de passer un signal temporel $x(t)$ du domaine réel au domaine complexe. Le signal peut ensuite s'écrire $x_{H}(t)=a(t)e^{j\phi(t)}$ où $a(t)$ est l'amplitude et $\phi(t)$, la phase. Cette transformation est particulièrement exploitée car le module de $x_{H}(t)$ permet de récupérer l'amplitude et la phase est obtenue en prenant l'angle de $x_{H}(t)$.

\subsubsection{Transformée en ondelettes}
La transformée en ondelettes \citep{tallon-baudry_oscillatory_1997, worrell_recording_2012} permet de décomposer un signal dans le domaine temps-fréquence. La décomposition en ondelettes d'une fonction $f$ est définie par:
\begin{equation}
f(a, b)=\int_{-\infty}^{\infty} \mathrm{f}(x)\overline{\psi}_{a, b}\mathrm{d}x 
\end{equation} \\
Où $\psi$ est appelé ondelette mère dont la définition générale est donnée par $\psi_{a,b}=\frac{1}{\sqrt{a}}\Psi(\frac{x-b}{a})$ où $a$ est le facteur de dilatation et $b$ le facteur de translation. Le choix de l'ondelette mère s'est porté sur l'ondelette de Morlet qui est très largement utilisée à travers la littérature et définie par:
\begin{equation}
w(t,f_{0})=A \mathrm{e}^{-t^{2}/2\sigma_{t}^{2}} \mathrm{e}^{2i\pi f_{0}t}
\end{equation}\\
Où $\sigma_{f}=1/2\pi \sigma_{t}$ et $A=(\sigma_{t}\sqrt{\pi})^{-1/2}$. L'ondelette de Morlet est caractérisée par le ratio constant $r=f_{0}/\sigma_{f}$ que nous avons fixé égale à $7$ comme suggéré par \cite{tallon-baudry_oscillatory_1997}.\\ 
Cette décomposition peut être comparée à la transformée courte de Fourier qui décompose le signal en une somme de combinaisons linéaire de sinus et de cosinus mais part du principe qu'il existe une régularité dans le signal permettant une telle décomposition. La transformée en ondelettes résout plusieurs limitations:
\begin{itemize}
\item Elle permet d'obtenir l'énergie d'un signal dans le temps, ce qui permet une bien meilleure exploration des phénomènes.
\item Le rapport constant $r$ permet d'obtenir des ondelettes dont la résolution fréquentielle varie    en fonction des fréquences et permet une meilleure coïncidence avec la définition des bandes physiologiques \citep{bertrand_time-frequency_1994}
\end{itemize}
\vspace{1\baselineskip}
Tout les attributs qui vont être maintenant présentés, utilisent les méthodes décrites ci-dessus.

\subsubsection{Évaluation statistique à base de permutations}
Pour une distribution de permutations construite à partir de deux sous-ensembles $A$ et $B$ et comportant $N$ observations et pour une valeur $p$ prédéfinie, on pourra conlure que:
\begin{itemize}
	\item $A>B$ si $A$ est parmi les $N-N\times p$ derniers échantillons (\textit{"One-tailed test upper tail"})
	\item $A<B$ si $A$ est parmi les $N\times p$ premiers échantillons (\textit{"One-tailed test lower tail"})
	\item $A\neg B$ si $A$ est soit inférieur aux $(N\times p)/2$ premiers échantillons soit supérieur aux $(N-N\times p)/2$
\end{itemize}  
\figScaleX{0.6}{stat_permutations}{\textit{"One-tailed"} et \textit{"two-tailed"} test}
Grâce à cette méthode d'évaluation statistique, nous pourrons par exemple conclure si l'on a une augmentation, une diminution ou une différence statistique entre une valeur de puissance et la puissance contenue dans une période de baseline. Dernière précision, on comprend ainsi que pour obtenir une valeur $p$ il faut que la taille de la distribution $N$ soit au moins de $1/p$.

\subsubsection{Hyperplan}
Un hyperplan est un espace de co-dimension 1. Donc, dans un espace $3D$, l'hyperplan est un plan (dimension $2D+1$). De manière générale, un espace de dimension $N$ possède un hyperplan de dimension $N-1$
\begin{equation}
	dim_{ESPACE} = dim_{HYPERPLAN}+1
\end{equation}

% ********************************************
%                 PUISSANCE
% ********************************************
\subsection{Puissance spectrale}

\subsubsection{Méthodes explorées}
Le calcul de la puissance spectrale a été approché par deux méthodologies et qui ont été utilisés à des fins différentes :
\begin{itemize}
	\item La transformée d'Hilbert: souvent exploité dans le cadre du décodage ainsi que pour garder une uniformité entre les attributs de phase et \pacFR basés eux aussi sur cette transformée.
	\item La transformée en ondelettes: principalement utilisée pour la visualisation des cartes \tf à cause de l'adaptation des ondelettes aux bandes physiologiques.
\end{itemize}

\subsubsection{Normalisation}
On utilise la normalisation pour observer l'émergence d'un phénomène par rapport à une période définie comme baseline. A travers la littérature, quatre grands types de normalisation sont rencontrés:
\begin{enumerate}
	\item Soustraction par la moyenne de la baseline
	\item Division par la moyenne de la baseline
	\item Soustraction puis division par la moyenne de la baseline
	\item Z-score: soustraction de la moyenne puis division par la déviation de la baseline
\end{enumerate}
La normalisation z-score est certainement la plus fréquemment rencontrée à travers la littérature. Le choix du type de normalisation dépend du type de données utilisées. Dans le cadre de nos données, \textit{3.} était clairement la plus adaptée pour la visualisation. En revanche, dans le cadre de la classification, nous obtenions systématiquement de meilleurs résultats sans normalisation.

\subsubsection{Évaluation statistique}
La fiabilité statistique de la puissance a été évaluée en comparant chaque valeur de puissance à la puissance contenue dans une période définie comme baseline. Pour ce faire, nous avons testé deux approches:
\begin{enumerate}
	\item Permutations : les valeurs de puissance et de baseline sont aléatoirement mélangées à travers les essais. Puis, on normalise cette puissance. En répétant cet procédure $N$ fois, on obtient une distribution qui peut ensuite être utilisée pour en déduire la valeur $p$ de la véritable puissance (cf: \textit{pré-requis})
	\item "Wilcoxon signed-rank test": ordonne les distances entre les paires de puissances (vraie valeur, baseline) \citep{demandt_reaching_2012, rickert_encoding_2005, waldert_hand_2008}
\end{enumerate}
\figScaleX{0.6}{ossandon_tf}{Exemple de représentation temps-fréquence de puissance normalisées z-score \citep{ossandon_transient_2011}}



% ********************************************
%                     PHASE
% ********************************************
\subsection{Phase}
L'extraction de la phase se fait de la m\^{e}me manière que pour le \PACFR, en prenant l'angle de la transformée d'Hilbert d'un signal filtré. La significativité peut être évaluée en utilisant le test de Rayleigh \citep{jervis_fundamental_1983, tallon-baudry_oscillatory_1997}. Point de vue pratique, cela correspond à la fonction \textit{circ\_rtest} de la toolbox Matlab \textit{CircStat} \citep{berens_circstat_2009}




% ********************************************
%                   PAC
% ********************************************
\subsection{\PACEN}
Le calcul du \PACEN ne se limite pas uniquement à la méthode. En réalité, pour obtenir une estimation fiable sur des données réelles, il est indispensable de suivre les trois étapes suivantes:
\begin{enumerate}
	\item Estimation de la véritable valeur de PAC. Il existe plusieurs méthodes.
	\item Calcul de "\textit{surrogates}": on va calculer des PAC déstructurés. Idem, il existe de nombreuses méthodes
	\item Correction du véritable PAC par les "\textit{surrogates}". Cette correction, qui est en faite une normalisation, aura pour but de soustraire à l'estimation du PAC de l'information considérée comme bruitée.
\end{enumerate}
Les sous-parties suivantes présenteront de manières succinctes les principales méthodes rencontrées dans la littérature, ainsi que différents types de corrections applicables.

\subsubsection{Méthodologie du \pacEN}
Il existe une large variété de méthodes pour calculer le PAC, ce qui complique son exploration. Toutefois, il n'existe pas de consensus sur une méthode plus polyvalente qu'une autre, chacune possédant ses points forts et limitations.
Pour aller un peu plus loin, et présenter quelques méthodes, il est nécessaire d'introduire quelques variable. Soit $x(t)$, une série temporelle de données de taille N. Pour cette série temporelle, on souhaite savoir si la phase extraite dans une bande de fréquence $f_{\phi}=[f_{\phi_{1}},f_{\phi_{2}}]$ est couplée avec l'amplitude contenue dans $f_{A}=[f_{A_{1}},f_{A_{2}}]$. Pour cela, on va tout d'abord extraire $x_{\phi}(t)$ et $x_{A}(t)$ les signaux filtrés dans ces deux bandes. Enfin, la phase $\phi(t)$ est obtenue en prenant l'angle de la transformée d'Hilbert de $x_{\phi}(t)$ tandis que l'amplitude $a(t)$ est obtenue en prenant le module de la transformée d'Hilbert de $x_{A}(t)$. 

\begin{enumerate}

	% MEAN VECTOR LENGTH
	\item \mvl: \\
	Cette méthode à été introduite par \cite{canolty_high_2006} et consiste à sommer, à travers le temps, le complexe formé de l'amplitude des hautes fréquences avec la phase des basses fréquences. L'équation est donnée par:
	\begin{equation}
	MVL = |\sum_{j=1}^{N} a_(j) \times e^{j\phi(j)}|
	\end{equation} \\
	
	% KULLBACK-LEIBLER DIVERGENCE
	\item \kld:\\
	A l'origine, la divergence de Kullback-Leibler (KLD), qui est issue de la théorie de l'information, permet de mesurer les dissimilarités entre deux distributions de probabilités. Ainsi, pour pouvoir utiliser cette mesure dans le cadre du PAC, \cite{tort_measuring_2010} propose une solution élégante qui consiste à générer une distribution de densité probabilités de l'amplitude (DPA) en fonction des valeurs de phase et d'ensuite utiliser le KLD pour comparer cette distribution à la densité de probabilité d'une distribution uniforme (DPU). Plus la DPA s'éloigne de la DPU, plus le couplage entre l'amplitude et la phase est consistant. \\
	Pour construire la DPA, l'astuce consiste à couper le cercle trigonométrique en N tranches (dans l'article il est proposé de couper en 18 tranches de 20°). Puis, si on prend l'exemple de la tranche $[0,20°]$, on va chercher tout les instants temporels où la phase prend des valeurs comprises entre $[0,20°]$ ($t, \phi(t) \subset [0,20°]$). On prend ensuite la moyenne de l'amplitude pour ces valeurs de $t$ et on répète cette procédure pour chacune des tranches de phase. On obtient ainsi la densité d'amplitudes en fonction des valeurs de phase. Il ne reste plus qu'à normaliser cette distribution par la somme des amplitudes à travers les tranches et on récupère une distribution de densité de probabilités. La figure \ref{fig:PAC_plot_Tort_2010} \citep{tort_measuring_2010} présente un exemple de DPA en fonction de tranches de phase. \\

	\figScaleX{0.6}{PAC_plot_Tort_2010}{Densité de probabilité d'une distribution d'amplitudes en fonction de tranches de phases}
	
	Le calcul de la divergence de Kullback-Leibler est ensuite appliqué pour mesurer les dissimilarités entre la DPA et la DPU et c'est cette mesure qui servira d'estimation du \pacFR:
	\begin{equation}
	D_{KL}(P, Q) = \displaystyle\sum_{j=1}^{N} P(j) \times \log{\frac{P(j)}{Q(j)}}
	\end{equation} \\
	où $P(j)$ est la densité de probabilité de $a(t)$ en fonction de $\phi(t)$ et $Q(j)$ est la densité de probabilité d'une distribution uniforme. \\
	
	% HEIGHT-RATIO
	\item \hr \\
	La méthode du \hr \citep{lakatos_oscillatory_2005} est extr\^{e}mement proche du \kld. En effet, l'amplitude sera binée de la même façon en fonction des tranches de phase. La mesure du PAC est ensuite donnée par:
	\begin{equation}
	hr=(f_{max}-f_{min})/f_{max}
	\end{equation} \\
	où $f_{max}$ et $f_{min}$ sont respectivement le maximum et le minimum de la de la densité de probabilité de l'amplitude en fonction des valeurs de phase. \\
	
	% NDPAC
	\item \ndpac \\
	Le \ndpac, qui n'est pas une des méthodes les plus fréquemment rencontrées, présente toutefois une avantage certain. En plus de fournir une estimation fiable du \pacFR, \cite{ozkurt_statistically_2012} démontre l'existence d'un seuil à partir duquel on peut considérer l'estimation du PAC comme étant statistiquement fiable. La beauté de cette méthode, c'est que ce seuil statistique, qui est une fonction de la valeur p désirée, ne dépend que de la taille de la série temporelle. Ce qui rend son utilisation particulièrement simple. \\
	Pour estimer le PAC, une des hypothèses ayant permis d'aboutir à ce seuil statistique est de devoir normaliser l'amplitude par un z-score dénotée $\tilde{a}(t)$. L'estimation du PAC est quasiment identique au MVL puisque c'est en réalité le carré de celle-ci. Enfin, pour une valeur p désirée, l'article introduit le seuil statistique: \\
	\begin{equation}
	x_{lim}=N \times [erf^{-1}(1-p)]^{2}
	\end{equation} \\
	
	où $erf^{-1}$ est la fonction d'erreur inverse. On déduira que l'estimation PAC est significative si et seulement si cette valeur est deux fois supérieur à ce seuil. \\
	
	% AUTRES
	\item Autres méthodes:
	Tout les algorithmes présentés ci-dessus ont été testés, implémentés et comparés. En complément, voici une liste non exhaustive d'autres méthodes existantes:
	\begin{itemize}
		\item \textit{Phase Locking Value (PLV)} \citep{cohen_assessing_2008, penny_testing_2008}: détournement du PLV proposé par \cite{lachaux_measuring_1999} qui mesure la synchronie de phase entre deux électrodes. Cette méthode va comparer la phase des basses fréquences avec la phase de l'amplitude des hautes-fréquences.
		\item \textit{Generalized Linear Model (GLM)} \citep{penny_testing_2008}: outil décrit comme adapté aux données courtes et bruitées.
		\item \textit{Generalized Morse Wavelets (GMW)} \citep{nakhnikian_novel_2016}: basée sur des ondelettes, semble particulièrement utile dans le cadre de l'exploration des données.
		\item \textit{Oscillatory Triggered Coupling (OTC)} \citep{dvorak_toward_2014, watrous_phase-amplitude_2015}: issue d'une détection de maximums des hautes fréquences.
	\end{itemize}

\end{enumerate}

% PERMUTATIONS ET NORMALISATION
\subsubsection{Correction du \pacEN et évaluation statistique}
Nous avons vu dans la section précédente différentes méthodes permettant de calculer un \PACFR. Toutefois, celui-ci peut être largement améliorée en faisant une estimation du PAC contenu dans le bruit des données. Une fois que cette estimation sera faite, on pourra retrancher ce PAC bruité à la valeur initiale. Tout comme il existe plusieurs méthodes de PAC, les équipes de recherche proposent à tour de rôle de nouvelles méthodes. Parmi elles, on peut citer:
\begin{itemize}
	\item \textit{Time-lag}: proposée par \cite{canolty_high_2006}, on introduit un délai sur l'amplitude compris entre $[f_{s},N-f_{s}]$ où $f_{s}$ est la fréquence d'échantillonnage et $N$ est le nombre de points de la série temporelle
	\item \textit{Shuffling des couples [phase,amplitude]}: ici, on mélange aléatoirement les essais de phase et d'amplitude \citep{tort_measuring_2010}
	\item \textit{Swapping temporel d'amplitudes (ou de phase)}: on mélange aléatoirement les essais d'amplitude puis on recalcule le PAC avec la phase originale \citep{bahramisharif_propagating_2013, lachaux_measuring_1999, penny_testing_2008, yanagisawa_regulation_2012} 
\end{itemize}
Ces trois méthodes produisent une distribution de \textit{surrogates}. On pourra ensuite appliquer un z-score à la véritable estimation en utilisant la moyenne et la déviation de cette distribution. Enfin, l'évaluation statistique se fait également à partir de cette distribution  (cf: \textit{pré-requis}) \\
A ma connaissance, il n'existe pas de comparatif entre ces corrections et je n'ai jamais rencontré d'articles mentionnant que l'on ne puisse pas combiner les méthodes de PAC avec les différentes corrections. En revanche, ce qui est relaté c'est que le \textit{time-lag} nécessite des données longues d\^{u} à l'introduction de ce délai temporel. 

% COMPARATIF
\subsubsection{Comparatif des méthodes}
\cite{penny_testing_2008} ont comparé plusieurs méthodes dont le \textit{MVL}, \textit{PLV} et le \textit{GLM} et \cite{tort_measuring_2010} ont complété cette étude avec d'autres méthodes (cf. \ref{comp_pac}). Enfin, \cite{canolty_functional_2010} a fait une review qui comprend un descriptif très instructif.

% VISUALISATION
\subsubsection{Représentation du \pacEN}
Comparée à la puissance, l'exploration du PAC peut s'avérer plus complexe d\^{u} à sa dimensionnalité plus grande. Il existe donc des outils et des méthodes destinées à simplifier cette exploration et à visualiser ces résultats. \\
Exemple concret, si on cherche à conna\^{i}tre les modulations de puissance contenue dans un signal, on peut représenter une carte \tf. Pour le PAC, idéalement on voudrait visualiser les phases, les amplitudes et le temps mais ces trois dimensions empêche une représentation simple. On peut donc avoir recours à différents types de représentations complémentaires:
\begin{itemize}
	\item Puissance phase-locked : cette représentation permet de faire émerger l'existence d'un couplage, pour une phase donnée, et d'observer sa durée. Pour cela, on aligne les phase en détectant le pic le plus proche de l'instant temporel étudié. On calcul les cartes \tf que l'on va ensuite moyenner après les avoir recalées de la même façon que les phases (\cad avec la même latence).
	\item Comodulogramme : pour une tranche temporelle définie, on représente les valeur de PAC pour différentes valeurs de phase et d'amplitude
\end{itemize}

\figScaleX{1}{hemptinne_2013}{\textbf{(A)} Exemple de cartes temps-fréquence phase locked sur le $\beta$, \textbf{(B)} Exemple de comodulogramme}

La figure \ref{fig:hemptinne_2013} \citep{hemptinne_exaggerated_2013} met en évidence que la représentation des cartes temps-fréquence phase-locked \textbf{(A)} est limitée d'une part, par la phase sur laquelle on choisit de recaler et d'autre part cette méthode est également limité par l'instant où l'on choisit de recaler. Pour la figure \textbf{(B)}, le calcul du PAC se faisant à travers la dimension temporelle, on a aucune idée de l'évolution du couplage dans le temps. \\

% ERPAC
\subsubsection{\PACEN: résolution temporel?}
Comment peut-on savoir si un ensemble de musiciens jouent ensemble, en rythme? L'approche traditionnelle consiste à dire que, en fonction de la prestation du groupe, on sera en mesure de dire si ils étaient en rythme ou non. Donc on focalise notre attention sur chaque instant du morceau et on analyse chaque note, chaque décalage. Cela signifie aussi que toute notre attention a été mobilisée par l'analyse du rythme et finalement, on passe à côté de la musique. Notre attention au détail nous a écarté du morceau global. On pourrait dire que l'on a écrasé la dimension temporelle du morceau. Une autre approche consiste à assister à toute les répétitions du fameux groupe. Ce faisant, on est capable de dire si d'une manière générale les musiciens ont tendance à jouer ensemble. Ainsi, le jour d'une représentation, toute notre attention peut rester uniquement sur le concert. On garde donc la dimension temporelle.\\ 
C'est par ce changement de positionnement face au problème de résolution temporelle que \cite{voytek_method_2013} introduit le \erpac . L'approche traditionnelle du PAC nécessitant de connaître un nombre de cycles afin d'en déduire l'existence ou non du couplage, et donc perdre la dimension temps, l'article propose de calculer le PAC à travers les essais (ou répétitions). Pour un jeu de données de M essais de longueur N, on extrait respectivement les phases et les amplitudes $\phi_{M}(t)$ et $a_{M}(t)$ puis, pour chaque point temporel, on calcul la corrélation à travers les essais (corrélation linéaire-circulaire \citep{berens_circstat_2009} qui se fait entre l'amplitude et des sinus/cosinus de la phase). Il en résulte une valeur de corrélation pour chaque instant et donc, de couplage. \\



\section{Apprentissage supervisé}
% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
%                           APPRENTISSAGE SUPERVISE
% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
Le travail effectué durant cette thèse s'est exclusivement porté sur l'apprentissage supervisé. Celui-ci consiste à apprendre à la machine à reconna\^{i}tre des événements qui ont été labellisé au préalable (cf. \ref{labellisation}). A contrario, l'apprentissage non supervisé laisse la machine apprendre par elle-m\^{e}me. En pratique, l'apprentissage se fait sur des attributs. Par exemple, pour différencier des chats et des chiens, ou pourra utiliser l'angle formé par le sommet des oreilles. Les attributs doivent contenir une information pertinente permettant de différencier les classes. Enfin, les algorithmes de classification vont se servir de ces attributs pour définir une frontière entre les classes étudiées. A ce stade, il semble important de préciser que l'utilisation des outils d'apprentissage machine peut s'orienter (globalement) suivant deux axes:
\begin{enumerate}
	\item Optimisation des attributs: on travail sur un raffinement des attributs afin que ceux-ci soient les plus performants possibles pour séparer les classes
	\item Optimisation des paramètres de classification: on considère une base de données comme étant fixe, définitive, optimale et l'on va faire varier les différents paramètres liés à l'apprentissage machine (classifeurs, cross-validation...). C'est le cas des compétitions \textit{BCI} où tout le monde travail sur une même base de données.
\end{enumerate}
Bien s\^{u}r, ces deux axes peuvent être cumulés. Dans le cadre de cette thèse, le machine learning a été utilisé comme outil de validation d'hypothèses donc essentiellement porté sur l'optimisation des attributs. Le raffinement des paramètres de classification a également été étudié, mais, au final, il ne constitue pas la majeure partie de l'étude.
\vspace{1\baselineskip}

Un schéma classique d'analyse peut-être décrit par:
\begin{enumerate}
	\item Labellisation des données
	\item Constitution de données d'entra\^{i}nement (\train) et de test (\test)
	\item Choix d'un classifieur puis entra\^{i}nement de celui-ci sur les données \train
	\item Test de ce classifeur entra\^{i}né sur les données \test et évaluation de la performance
	\item Évaluation statistique de cette acuité de décodage
\end{enumerate}

% ********************************************
%              LABELLISATION
% ********************************************
\subsection{Labellisation et apprentissage}\label{labellisation}
La labellisation c'est le fait d'associer à chaque événement l'appartenance à une classe ou à une condition. C'est par ce procédé que l'on va pouvoir apprendre ensuite au classifieur à identifier les classes. Par exemple, considérons \textit{up} et \textit{down} deux classes qui reflètent des mouvements de la main vers le haut ou vers le bas. On va donc construire un vecteur $y_{direction}$ qui labellise chaque essais avec direction effectuée (ce vecteur peut aussi \^{e}tre booléen ou contenir des entiers. L'essentiel est que à chaque classe soit attribué une valeur qui lui est propre). Ce vecteur $y$ est appelé \textit{vecteur label}, qui vient labelliser chaque essais d'un vecteur d'attributs $x$. \\ 
\begin{equation}
	y_{direction}=
	\begin{pmatrix}
		up \\
		down \\
		down \\
		\vdots \\\
		up
	\end{pmatrix},
	y_{bool}=
	\begin{pmatrix}
		0 \\
		1 \\
		1 \\
		\vdots \\\
		0
	\end{pmatrix},
	x=
	\begin{pmatrix}
		x_{trial_{1}} \\
		x_{trial_{2}} \\
		x_{trial_{3}} \\
		\vdots \\
		x_{trial_{N}} \\
	\end{pmatrix}
\end{equation}
D'où le nom apprentissage supervisé. Finalement, l'apprentissage machine se fera gr\^{a}ce à ce vecteur label $y$ et cette matrice d'attributs $x$. Ce qui nous amène directement aux notions de \textit{training set} et de \textit{testing set}.\\
\figScaleX{0.5}{clf_labeling}{Labellisation de données}


% ********************************************
%           TRAINING ET TESTING
% ********************************************
\subsection{\textit{Training}, \test et \cvfr}
Cette section est sans aucun doute la plus importante pour le machine learning puisque c'est elle qui assure la conformité méthodologique. \\
Un bon exemple pour comprendre cette partie est celui des contr\^{o}les de mathématiques. Avant l'examen, l'étudiant s'entra\^{i}ne sur une série d'exercices. C'est la phase de \train. D'ailleurs, plus il s'entra\^{i}ne, plus ses chances de réussir à l'examen sont grandes. Le jour du contr\^{o}le, le professeur teste l'étudiant sur une série de nouveaux exercices en lien avec ce qu'il a étudié. C'est le \test. Ici, c'est un test parfait puisque l'étudiant est naïf sur le contenue de l'examen ce qui veut dire que l'on teste ses capacités mathématiques pures. Toutefois, il peut arriver durant la scolarité que l'on soit testé sur des exercices que l'on a déjà vu dans la phase de \train. Dans ce cas, la moyenne des notes des étudiants est généralement beaucoup plus élevée puisque l'on ne teste plus des capacités mathématiques, mais la capacité à restituer un apprentissage.

\subsubsection{\textit{Training set}, \textit{testing set} et naïveté}
Pour en revenir à la question du machine learning, on définit un une partie des données pour entraîner la machine. Ensuite, on teste cette machine entra\^{i}née sur un nouveau jeu de données de test. Il est essentiel d'avoir une séparation stricte entre des données définies comme \train et des données de \test afin d'assurer la naïveté du classifieur. M\^{e}me si cela peut par\^{i}tre évident, nous verrons que ça n'est pas toujours aussi facile que ça. \\
Se pose maintenant la question de comment l'on choisit de couper les données en \train et \test. Une méthode serait de prendre une partie des données de manière aléatoire, de la définir comme \train et sur tester sur les données restantes. Toutefois, ce choix ne représenterai qu'une partie des données. Une méthode plus exhaustive et plus rigoureuse consiste à utiliser une \cvfr (ou \cv).

\subsubsection{Validation-croisée}
La \cvfr (CV) est une procédure permettant de séparer les données en \train et \test. Pour comprendre comment cela fonctionne, prenons un ensemble composé de $N$ échantillons. Il existe plusieurs de CV mais de manière générale, toutes dérivent du même principe qui est la \cv \kfold \citep{efron1994introduction, kohavi1995study}. On coupe les $N$ échantillons en $k$ paquets de tailles égales (ou proches). Ensuite, le classifieur est entraîné sur $k-1$ paquets puis on le test sur le paquet restant. Cette procédure est ensuite appliquée $k$ fois afin que chaque paquet passe au \test. On dira que la \cv est \textit{stratified} si la proportion de classes représentées au sein de chaque dossier est approximativement uniforme à travers les folds. on pourra aussi rencontrer le terme \textit{shuffle} si il y a un mélange supplémentaire. Tout cela nous emmène à des CV k-fold, k-fold stratified, k-fold shuffle ou encore k-fold stratified shuffle. \\
Concernant le nombre de folds, on rencontre en générale 3 valeurs à travers la littérature: 3-folds, 5-folds ou 10-folds \citep{latinne2001limiting, yanagisawa_neural_2009, besserve_classification_2007, waldert_hand_2008}. Un cas particulier, mais si le nombre de folds $k=N$, ça revient à entraîner la machine sur $N-1$ échantillons tester sur celui qui a été isolé et on répète cette procédure $N$ fois. C'est ce que l'on appelle le \textit{\loo}. Toutefois cette dernière possède une grande variance et peut conduire à des estimations non fiables \citep{efron1994introduction, kohavi1995study}. \\
Un autre cas particulier, est celui du \textit{Leave-p-Subject-Out} \citep{vidaurre2009time, lajnef_learning_2015} qui consiste à entraîner sur $p$ sujets et tester sur les sujets restants. Cette procédure est particulièrement exigeante puisqu'elle nécessite d'avoir une certaine reproductibilité entre les sujets. Cette \cvfr est fréquente avec des données EEG mais impossible à mettre en  \oe{}uvre pour la sEEG à cause de l'implantation unique de chaque sujet. 
\figScaleX{1}{clf_cv}{Exemple d'une cross validation 3-folds}


% ********************************************
%                 CLASSIFIEURS
% ********************************************
\subsection{Classifieurs}
\begin{enumerate}

	% LINEAR DISCRIMINANT ANALYSIS
	\item \lda (LDA) \\
	Le LDA \citep{fisher_use_1936} est un classifieur linéaire. Pour un problème à deux classes, le LDA tente de trouver un hyperplan qui va maximiser la distance entre les classes tout en minimisant la variance inter-classes. Ce classifieur fait l'hypothèse que les données sont normalement distribuées avec la même co-variance. Un problème multi-classes pouvant être transformée en multiple bi-classes, le LDA tente de trouver un hyperplan séparant la classe du reste (\textit{One-vs-All}) \\	
	\figScaleX{0.4}{LDA_Lotte_2007}{Principe du \lda  \citep{lotte_review_2007}}
	
	% SUPPORT VECTOR MACHINE
	\item \svm (SVM)\\
	Le SVM \citep{boser_training_1992, cortes_support-vector_1995, vladimir1995nature} utilise également un hyperplan pour séparer deux classes. Toutefois, cet hyperplan optimal est trouvé en maximisant les marges (ou distance) entre ce plan et les attributs les plus proches. Le SVM possède une particularité, il utilise un noyau qui peut permettre de résoudre les problèmes linéaire (\textit{linear SVM}) mais également les problèmes non-linéaire en projetant les données dans un espace de dimension supérieure (\textit{kernel trick}). Un noyau que l'on retrouve assez régulièrement est le \textit{Radial Basis Function (RBF)} \citep{burges1998tutorial}. Les problèmes multi-classes peuvent également être traités en \textit{One-vs-All}
	\figScaleX{0.4}{SVM_Lotte_2007}{Principe du \svm \citep{lotte_review_2007}}
	
	% K-NEAREST NEIGHBOR
	\item \knn (KNN) \\
	Pour un nouveau point de testing, le KNN \citep{fix1951discriminatory} mesure la distance avec les $k$ plus proches voisins et déduit la classe de ce point en fonction des classes de ces k-voisins (l'attribution de la classe se fait donc par vote)
	\figScaleX{0.4}{knn}{Principe du \knn \citep{weinberger2005distance}}
	
	% NAIVE BAYES
	\item \nb (NB) \\
	Le NB \citep{fukunaga1990introduction} est un classifieur probabiliste. Une des hypothèses du NB est que les données dans les classes doivent être normalement distribuées et indépendante.
\end{enumerate}

La figure \ref{comp_clf} en annexe, issue de l'excellentissime librairie python scikit-learn dédiée au machine learning, illustre le comportement de chaque classifieur façe à trois types de données. D'autres informations détaillées à propos des classifieurs peuvent être trouvées dans  \cite{lotte_review_2007, wieland_performance_2014, wu_top_2008}
\figScaleX{0.9}{clf_classifier}{Entraînement puis test d'un classifieur linéaire}

% ********************************************
%               DECODING ACCURACY
% ********************************************
\subsection{Évaluation de la performance de décodage}
La question qui se pose maintenant, c'est comment évaluer la performance de décodage. Pour cela, on peut par exemple utiliser le \textit{\da} ou le \textit{roc}
\begin{enumerate}
	\item \da (DA)\\
	L'utilisation (DA) est ce que l'on retrouve le plus fréquemment. Le calcul est simple, on compare les véritables labels avec les labels prédits par le classifieur. En faisant la somme des labels correctement prédits divisé par le nombre d'essais, on obtient un ratio qui correspond à l'acuité de décodage. Le plus souvent, ce ratio est ensuite exprimé en pourcentage. Le taux d'erreurs peut-être calculé en prenant $1-DA$. 
	\figScaleX{0.2}{clf_da}{Calcul de l'acuité de décodage}

	\item \roc (ROC) \\
	Une autre méthode pour évaluer la performance de décodage est l'utilisation de l'aire sous la courbe (AUC) ROC \citep{ling_auc_2003,huang_using_2005,bradley_use_1997}. Celle-ci prend en compte le nombre d'essais correctement et incorrectement classifiés et pourrait donc prendre davantage de valeur possible comparé au \da.
\end{enumerate}


% ********************************************
%                STATISTIQUE
% ********************************************
\subsection{Seuil de chance et évaluation statistique de la performance de décodage}
\label{chance_level_stat}
De manière théorique, le seuil de chance est donné par $1/c$ où $c$ est le nombre de classes. Par exemple, un problème à quatre classes donne un seuil de chance de $25\%$. Toutefois, ce seuil de chance est atteint pour un nombre de sample $n$ infinis. En pratique, nous travaillons avec un nombre réduis de données, parfois même, avec très peu de sample. Dans ce cas, on peut obtenir des DA très élevés qui pourtant, ne sont pas pertinents. Les méthodes présentées ci-dessous ont pour but de trouver le seuil de chance associé à un jeu de donnée et de trouver pas la même occasion, la valeur $p$.
\begin{enumerate}
	\item Loi binomiale \\
	En faisant l'hypothèse que l'erreur de classification suit une distribution binomiale cumulative, on peut utiliser la loi suivante pour en déduire la probabilité de prédire au moins $z$ fois la classe $c$:
	\begin{equation}
		P(z)=\sum_{i=z}^{n}
		\begin{pmatrix} n \\ i \end{pmatrix} \times 
		\left(\frac{1}{c}\right)^{i} \times 
		\left(\frac{c-1}{c}\right)^{n-1}
	\end{equation}
	\item Permutation \\
	Les permutations présentent l'avantage d'être calculées à partir des données (\textit{data driven}).   \cite{ojala_permutation_2010} nous renseigne sur les différents types de permutations possibles dans le cadre du décodage:
	\begin{enumerate}
		\item \textit{Full permutation}: les données sont mélangés
		\item \textit{Shuffle y}: le vecteur de label est mélangé. C'est la procédure la plus fréquemment rencontrée.
		\item \textit{Intra-class shuffle}: les données sont mélangées à travers la dimension $features$ (colonne) et ce, à l'intérieur de chaque classe.
	\end{enumerate}
	Autant les méthodes $(a)$ et $(b)$ nous renseigne véritablement sur la consistance d'un décodage par rapport aux données, autant la méthode $(c)$ donne des informations un peu différentes. En effet, en cas de décodage non-significatif, on pourra soit conclure qu'il n'y a pas de consistance dans les attributs à l'intérieur des classes, soit que le classifieur est incapable d'utiliser cette l'inter-dépendance. \cite{ojala_permutation_2010} précise que dans ce cas, il n'est pas nécessaire d'utiliser un classifieur compliqué et qu'un classifieur simple devrait suffire.
\end{enumerate}

Cette partie est volontairement synthétique puisqu'elle a fait l'objet d'une publication scientifique (cf. \ref{seuil_chance}). \\
Un pipeline standard de classification est proposé en annexe (cf. \ref{clf_pip}).

% -> Single et Multi-features
\subsection{Du single au multi-features}
% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
%                       SINGLE ET MULI-FEATURES
% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
Dans les sections précédentes, nous avons vu comment extraire des attributs de l'activité neuronale et comment les classifier. C'est ce que l'on appelle le \textit{single feature} (SF), \cad que l'on évalue la performance de chaque attribut séparément. Cette approche permet de constituer un set de features pertinents et répond à des questions neuro-scientifique. Cette démarche de SF a donc un but exploratoire. \\
La question que l'on peut maintenant se poser, c'est quelle performance de décodage puis-je obtenir si je combine ces attributs et dans quel cas est-ce utile? C'est le multi-attributs (ou \textit{multi-features} (MF)). Tout d'abord, le MF est utilisé lorsqu'il y a soit un désir soit un besoin de performances accrue. Par exemple, on utilisera le MF dans les compétitions de décodage ou tout simplement, pour une BCI où la performance est essentielle. Si l'on construit un système de bras robotisé piloté par activité neuronale, on comprend sans peine que celui-ci doit être le plus efficace possible et donc, le MF s'impose. Le dernier cas où l'on rencontre du MF, et ce n'est pas le cas le plus glorieux, c'est le cas où il y a un besoin de pallier à des résultats de SF assez faibles. La littérature expose des \da toujours plus hauts, des méthodes toujours plus complexes et donc, pour publier correctement un article, il faut avoir des résultats au-moins aussi perspicaces. \\
Le \mf c'est donc l'utilisation de multiples attributs pour aboutir à une classification et ce, sans sélection particulière. Individuellement, les attributs d'un même set n'auront pas la même performance. Certains seront des bons marqueurs et d'autres, n'ajouteront pas ou peu d'information. Donc en combinant ces features, il est probable que l'acuité de décodage soit moins bonne que la performance en attribut unique. Pour cela, on pourra donc utiliser des algorithmes de sélections de marqueurs (\textit{feature selection}). Le but de cette sélection est de trouver dans un set d'attributs, un sous-ensemble dont la performance groupée est meilleure que la performance individuelle. \\
Cette sélection est une procédure exigeante où le risque de sur-apprentissage est grand. C'est la raison pour laquelle cette sélection doit être mise à l'intérieur d'une \cv. Donc on définis un set de \train et de \test grâce à la validation croisée, puis sur le \train, on lance la \textit{feature selection}. On aboutit à un sous-ensemble de marqueurs qui va servir à entraîner le classifieur. Ensuite, on sélectionne ce subset dans le \test et on test le classifieur avec ce subset. Toute ceci étant enfin répété pour chaque \textit{fold} de la \cv. A la vue de cette procédure, deux problèmes émergent:
\begin{itemize}
	\item La sélection d'attributs se faisant à l'intérieur des folds de la \cv, on peut très bien aboutir à des listes d'attributs différentes. Pour obtenir une information finale, on pourra donc parler des attributs les plus fréquemment choisis. Par exemple, si la sélection se fait dans un \cv 10-folds, on pourra dire que le feature 1 a été choisi 7/10, le feature 2, 3/10... 
	\item En fonction de la sélection choisie et de la \cv, le pipeline complet peut être très (très) lourd et long.
\end{itemize}
\vspace{1\baselineskip}
Les mécanismes de \textit{feature selection} peuvent être regroupés en deux grandes familles \citep{guyon_introduction_2003, liu_classification_2008, das_filters_2001}: les \textit{Filter methods} et les \textit{Wrapper methods}.

\subsubsection{\textit{Filter methods}}
Ces méthodes sont basées sur un critère et sont indépendantes du classifieur. Parmi elles, on retrouve des outils de corrélation, d'information mutuelle ou encore de statistiques. Ces derniers outils évaluent la contribution de chaque feature de manière indépendante sans tenir compte de la corrélation entre ces features. Pour résoudre ce problème, \cite{yu_redundancy_2004, ding_minimum_2005} introduisent le \textit{minimal-redundancy-maximal-relevance} qui en plus de trouver les features les plus pertinents, va permettre d'éliminer ceux qui sont redondants. \\
Pour terminer, ces méthodes sont effectivement indépendantes de l'algorithme de classification mais elles peuvent s'avérer optimales pour tel ou tel classifieur (ex: l'utilisation du critère de Fisher pour filtrer les features est très performant lorsqu'il est ensuite associé au \lda \citep{duda2001pattern}).

\subsubsection{\textit{Wrapper methods}}
Contrairement aux méthodes de filtrage, les \textit{wrapper} utilisent le classifieur comme outil de sélection. Le premier inconvénient que l'on peut d'ors et déjà leur reprocher, c'est que le résultat final sera donc classifieur-dépendant, donc difficile pour la généralisation. \\
Parmi ces \textit{Wrapper methods}, on peut citer:
\begin{enumerate}
	\item Sélection exhaustive: on teste toutes les combinaisons de features possibles puis on sélectionne la meilleure. Procédure qui ne peut être faisable qu'en présence d'un jeu de données particulièrement restreint.
	\item Sélection sur la statistique de décodage: on utilise le classifieur pour évaluer l'acuité de décodage de chaque feature séparément pour en déduire une valeur p (cf: \ref{chance_level_stat}). Enfin, on sélectionne les features dont la valeur p est inférieur à un seuil désiré.
	\item Sélection séquentielle: processus où l'on va ajouter/enlever des features de manière séquentielle jusqu'à atteindre un décodage optimal. Ce type de sélection se fait suivant deux directions:
	\begin{enumerate}
		\item \ffs (FFS): la première étape consiste à évaluer la performance de chaque attributs. On sélectionne le meilleur que l'on va ensuite combiner en couple avec tout les features restant. On sélectionne le meilleur couple puis on teste les combinaisons des meilleures triplettes... On continu tant que la performance s'améliore. Si le DA d'une étape $i$ est inférieur au DA de l'étape $i-1$, on considère le nouveau subset de features à $i-1$.
		\figScaleX{1}{clf_forward}{Exemple d'une \ffs appliquée sur six features}
		\begin{center} \rule{5cm}{0.2pt} \end{center}
		
		\item \bfe (BFE): la philosophie est la même que pour un \textit{forward}. On classifie d'abord les $N$ features pris ensemble, puis on enlève à tour de rôle chaque marqueur. On sélectionne le subset composé de $N-1$ features ayant fournit le meilleur résultat, puis on enlève de nouveau chaque feature... L'algorithme s'arrête de la même façon que le \textit{forward}.  
		\figScaleX{1}{clf_backward}{Exemple d'une \bfe appliquée sur six features}
		\begin{center} \rule{5cm}{0.2pt} \end{center}
	\end{enumerate}
	De manière générale, il est rapporté que la FFS converge plus rapidement que la BFE \citep{guyon_introduction_2003}. Toutefois, la FFS tombe plus facilement dans des minimums locaux et donc, mène à un décodage moins bon. En effet, la \textit{forward} sélectionne pas-à-pas les meilleurs attributs, elle est donc moins ensembliste que la \textit{backward}.
\end{enumerate}

\vspace{1\baselineskip}
Les méthodes de filtrage demandent moins de ressources et représentent donc un premier choix pour les larges sets de données. En revanche, elles peuvent ne pas déceler les phénomènes de complémentarité entre features. Pour cette dernière raison, les méthodes de wrapper fournissent en général de meilleurs résultats \citep{chai_evaluation_2004}.


% -> Généralisation temporelle
\subsection{Généralisation temporelle}
L'introduction du \textit{single-feature} faite plus haut était une présentation générique, \cad que celle-ci est vraie quelque soit les features étudiés. On pourra donc classifier des attributs de puissance, de PAC, de phase, d'entropie... On peut également envisage l'étude un seul marqueur mais dans sa dimension temporelle. En effet, cela consiste à entraîner et tester un classifieur à différents instants temporels pour voir si le décodage varie dans le temps. Une des limitations de cette utilisation d'un classifieur est que, à chaque instant, celui-ci change. Donc on ne peut inférer aucune généralisation. Pour envisager une généralisation, il faut entraîner le classifieur à un instant puis le tester à travers toute la dimension temporelle restante. Dans ce cas, on pourra parler de généralisation mais reste encore le problème du choix de l'instant temporel qui servira à entraîner le prédicateur.\\
\figScaleX{0.55}{waldert_timeresolved}{Exemple de décodage temporel \citep{waldert_hand_2008}. Ici, l'auteur décode 4-directions de mouvements de la main dans le temps. A chaque instant, un classifieur est créé, entraîné puis testé à ce même instant.}

Pour répondre à cette question, \cite{king_characterizing_2014} introduisent une idée particulièrement esthétique visant à généraliser le comportement d'un classifieur à travers le temps. Sans trop de surprise, ils ont nommé cette méthode la \textit{généralisation temporelle}. Elle permet de répondre à deux limitations:
\begin{itemize}
	\item Comment généraliser le comportement d'un classifieur lors d'une étude temporelle?
	\item Comment choisir l'instant qui servira à entraîner le classifieur et quel impact ce choix aura-t-il sur le reste du décodage temporel? 
\end{itemize}
La méthodologie consiste à prendre un instant $i$, entraîner le classifieur et tester celui-ci sur tout les instants. Puis, on prend l'instant suivant $i+1$, on entraîne un nouveau classifieur et on teste... Et on répète cette procédure pour tout les instants. On obtient ainsi une représentation $2D$ où, par convention, l'axe des ordonnées matérialise l'endroit où le classifieur a été entraîné (\textit{Training time}) et l'axe des abscisses pour tester ce prédicateur sur le reste de la dimension temporelle (\textit{Generalization time}). La couleur permettra de signaler la performance de décodage.

\figScaleX{0.5}{king_timegene}{Exemple de généralisation temporelle \citep{king_characterizing_2014}}

\todo[color=orange!100]{Il y a aussi des interprétations neuro-scientiques possibles. Comme le fait d'entraîner dans une phase et de tester dans une autre. Si ça marche, ça veut aussi dire que ces phases partagent des corrélats neuronaux. A voir si j'inclue ceci.}
%D'un point de vue neuro-scientifique, la \textit{généralisation temporelle} apporte également son lot d'informations complémentaires, même si, l'interprétation n'est pas toujours évidente. Si l'on imagine tâche au cours de laquelle se succèdent des phases (eg. Phase de repos/préparation motrice/exécution motrice). Ce n'est pas très étonnant si on entraîne


% ********************************************
%              CONCLUSION
% ********************************************
\section{Configuration pour débuter}
Dans la jungle des méthodes, il peut parfois être difficile de s'y retrouver. Cette section a pour objectif de fournir une liste de méthodes conseillées pour débuter. Rien ne dit que ce sont les meilleures méthodes mais elles ont le mérite d'avoir fait leurs preuves, que ce soit dans cette thèse et surtout, dans la littérature. Gardons à l'esprit que les meilleures méthodes dépendent des données mais certaines, sont plus polyvalentes.

\vspace{1\baselineskip}

\todo[inline,caption={},color=blue!20]{
	\textbf{Couplage phase-amplitude :} \\
	\kld avec swapping des essais de phase et d'amplitude \citep{tort_measuring_2010}
}

\vspace{1\baselineskip}

\todo[inline,caption={},color=green!20]{
	\textbf{Classification :} \\
	\begin{enumerate}
		\item Cross-validation: 10-folds
		\item Classifieur: \svm avec noyau linéaire \citep{vladimir1995nature,lotte_review_2007}
		\item Évaluation statistique: permutations \citep{ojala_permutation_2010,combrisson_exceeding_2015}
		\item Multi-features: \bfe \citep{guyon_introduction_2003}
	\end{enumerate}
}

% ********************************************
%              BRAINPIPE
% ********************************************
\section{Implémentation des méthodes présentées}
A force d'implémenter différentes méthodes afin de les tester, il se trouve que cet amas de fonction a formé assez naturellement une \textit{toolbox}. Avec un peu de mise en forme, une uniformisation à travers les fonctions et la constitution d'une aide, nous avons décidé de mettre cet ensemble d'outils \textit{online} et en libre accès. \\
La toolbox s'appelle \brainpipe, entièrement codée en Python. Le choix de python a également été mûrement réfléchis. La première version était en Matlab. Toutefois, Python présente les avantages d'être libre, beaucoup plus souple et intuitif et bien plus puissant. De plus, la communauté s'est appropriée ce langage et est particulièrement réactive. Quant à la documentation, elle est excellente et permet d'apprivoiser ce langage très rapidement. Autre point fort qui est une conséquence du libre, c'est la multitudes d'outils existants. Par exemple, pour la classification, il existe \textit{scikit-leanr} dont la qualité est sans comparaison possible avec Matlab. Enfin, les derniers outils à la pointe (tel que \textit{tensorflow} de Google) ne seront jamais proposé en Matlab. Le Julia (qui est un autre langage de programmation) semble particulièrement prometteur mais, étant très jeune et ne disposant que d'une petite communauté, il a été écarté. Autre point, les outils que nous avons présenté dans cette partie méthode peuvent nécessiter énormément de ressources. \brainpipe a donc entièrement été développé pour le calcul parallèle qui est intégré par défaut. A titre de comparaison, le calcul du PAC sur tout un jeux de données a pris une semaine Matlab, contre une journée en Python.

\vspace{1\baselineskip}
\brainpipe est dédiée à la classification de signaux cérébraux. La construction globale peut être résumée avec les points suivants:
\begin{itemize}
	\item \textit{Pre-processing}: pré-traitement de données (filtrage, re-référencement, informations anatomiques en fonction de coordonnées MNI...)
	\item \textit{Features}: extraction d'une large variété de features (amplitude, puissance, phase, PAC, PLV, ERPAC, entropie...)
	\item \textit{Classification} des signatures cérébrales: repose sur \textit{scikit-learn} pour l'implémentation des algorithmes de classification. \brainpipe sert à faire le lien entre l'analyse de données neuro-scientifiques et le \textit{machine-learning}. Il y a également un module dédié au \textit{multi-features}.
	\item \textit{Statistics}: tout les attributs disposent des méthodes statistiques ainsi que pour la classification et la correction multiple. 
	\item \textit{Visualization}: enfin, un ensemble d'outils de visualisation sont également disponible afin de faciliter les représentations graphiques.
\end{itemize}

\vspace{1\baselineskip}
Liens vers \brainpipe:
\begin{itemize}
	\item Téléchargement: \url{https://github.com/EtienneCmb/brainpipe}
	\item Documentation en ligne: \url{https://etiennecmb.github.io/}
\end{itemize}


\begin{enumerate}
	\item \textit{ipywksp}: pour les personnes désirant ou utilisant déjà python, un environnement de développement particulièrement agréable et puissant s'impose de plus en plus. Ce projet s'appelle 
	\href{http://jupyter.org/}{Jupyter}. Pour l'introduire brièvement, c'est un notebook dans lequel le code est intégré dans des cellules qui peuvent être lancées individuellement. Une des lacunes de ce notebook pour les utilisateurs Matlab, c'est le manque d'un \textit{workspace} pour voir les variables en cours. \textit{ipywksp} est un petit module, également développé dans cette thèse qui vient combler ce manque. Toujours en libre accès: \url{https://github.com/EtienneCmb/ipywksp}  
	\item \textit{visbrain}: enfin, un dernier module Python qui est en cours de développement mais qui proposera une interface graphique pour toute visualisation nécessitant un cerveau MNI $3D$ (connectivité, projection corticale...). Il existe de nombreux logiciels existants déjà. Le but de celui-ci sera de permettre une intégration totale avec \brainpipe. Le projet débute, mais le lien suivant permet de suivre son évolution: \url{https://github.com/EtienneCmb/visbrain} 
\end{enumerate}


