% #############################################################################
%                             METHODOLOGIE
% #############################################################################
\chapter{méthodologie}

Cette partie méthodologique sera divisée en deux grandes sous parties visant à présenter :
\begin{enumerate}
	\item L'extraction des features: présentation des méthodes utilisées dans le cadre de l'extraction d'attributs issus de l'activité neuronale. De manière générale, nous avons étudiés des attributs spectraux comprenant:
	\begin{itemize}
		\item Phase et puissance spectrale
		\item Attributs de couplage
	\end{itemize}
	\item Le machine learning: présentation des principaux algorithmes testées dans le cadre du décodage de l'activité neuronale 
\end{enumerate}


\section{Extraction des features}
% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
%                                   FEATURES
% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
Comme nous l'avons décrit précédemment, l'objectif du décodage de l'activité neuronale est d'arriver à extraire des signaux cérébraux une information suffisamment pertinente pour pouvoir discriminer différents types de classes (exemple: mouvement vers la gauche Vs droite). \\
Tout les attributs testés dans le cadre de cette thèse sont des attributs spectraux, donc issus de bandes de fréquences. La plupart de ces outils partagent donc une partie méthodologique commune à savoir, le filtrage. De plus, la plupart sont extraits en utilisant la transformée d'Hilbert. Pour éviter une redondance à travers les attributs, nous allons tout d'abord introduire quelques pré-requis.

\subsection{Pré-requis}
% ********************************************
%               PRE-REQUIS
% ********************************************
\subsubsection{Filtrage}
L'intégralité des filtrages dans cette thèse ont été effectués avec la fonction \textit{eegfilt} (qui a ensuite été reproduite pour le passage à python). De plus, afin d'éviter tout phénomène de déphasage, le fonction \textit{filtfilt} a été systématiquement utilisée afin que le filtre soit appliqué dans les deux sens. Si cette dernière fonctionnalité n'est pas forcément indispensable dans le cadre d'un calcul de puissance, elle est absolument nécessaire pour un calcul de \pacFR.  \\
L'ordre du filtre présenté au dessus dépend de la fréquence de filtrage. Il a systématiquement été calculé en utilisant la méthode décrite par \cite{bahramisharif_propagating_2013}:
\begin{equation}
FiltOrder = N_{cycle} \times f_{s}/f_{oi}
\end{equation} \\
où $f_{s}$ est la fréquence d'échantillonnage, $f_{oi}$ est la fréquence d'intérêt et $N_{cycle}$ est un nombre de cycles définit par $N_{cycle}=3$ pour les oscillations lentes et $N_{cycle}=6$ pour les oscillations rapides. 

\subsubsection{Transformée d'Hilbert}
Transformée permettant de passer un signal temporel $x(t)$ du domaine réel au domaine complexe. Le signal peut ensuite s'écrire $x_{H}(t)=a(t)e^{j\phi(t)}$ où $a(t)$ est l'amplitude et $\phi(t)$, la phase. Cette transformation est particulièrement exploitée car le module de $x_{H}(t)$ permet de récupérer l'amplitude et la phase est obtenue en prenant l'angle de $x_{H}(t)$.

\subsubsection{Transformée en ondelettes}
La transformée en ondelettes \citep{tallon-baudry_oscillatory_1997, worrell_recording_2012} permet de décomposer un signal dans le domaine temps-fréquence. La décomposition en ondelettes d'une fonction $f$ est définie par:
\begin{equation}
f(a, b)=\int_{-\infty}^{\infty} \mathrm{f}(x)\overline{\psi}_{a, b}\mathrm{d}x 
\end{equation} \\
Où $\psi$ est appelé ondelette mère dont la définition générale est donnée par $\psi_{a,b}=\frac{1}{\sqrt{a}}\Psi(\frac{x-b}{a})$ où $a$ est le facteur de dilatation et $b$ le facteur de translation. Le choix de l'ondelette mère s'est porté sur l'ondelette de Morlet qui est très largement utilisée à travers la littérature et définie par:
\begin{equation}
w(t,f_{0})=A \mathrm{e}^{-t^{2}/2\sigma_{t}^{2}} \mathrm{e}^{2i\pi f_{0}t}
\end{equation}\\
Où $\sigma_{f}=1/2\pi \sigma_{t}$ et $A=(\sigma_{t}\sqrt{\pi})^{-1/2}$. L'ondelette de Morlet est caractérisée par le ratio constant $r=f_{0}/\sigma_{f}$ que nous avons fixé égale à $7$ comme suggéré par \cite{tallon-baudry_oscillatory_1997}.\\ 
Cette décomposition peut être comparée à la transformée courte de Fourier qui décompose le signal en une somme de combinaisons linéaire de sinus et de cosinus mais part du principe qu'il existe une régularité dans le signal permettant une telle décomposition. La transformée en ondelettes résout plusieurs limitations:
\begin{itemize}
\item Elle permet d'obtenir l'énergie d'un signal dans le temps, ce qui permet une bien meilleure exploration des phénomènes.
\item Le rapport constant $r$ permet d'obtenir des ondelettes dont la résolution fréquentielle varie    en fonction des fréquences et permet une meilleure coïncidence avec la définition des bandes physiologiques \citep{bertrand_time-frequency_1994}
\end{itemize}
\vspace{1\baselineskip}
Tout les attributs qui vont être maintenant présentés, utilisent les méthodes décrites ci-dessus.

\subsubsection{Évaluation statistique à base de permutations}
Pour une distribution de permutations construite à partir de deux sous-ensembles $A$ et $B$ et comportant $N$ observations et pour une valeur $p$ prédéfinie, on pourra conlure que:
\begin{itemize}
	\item $A>B$ si $A$ est parmi les $N-N\times p$ derniers échantillons (\textit{"One-tailed test upper tail"})
	\item $A<B$ si $A$ est parmi les $N\times p$ premiers échantillons (\textit{"One-tailed test lower tail"})
	\item $A\neg B$ si $A$ est soit inférieur aux $(N\times p)/2$ premiers échantillons soit supérieur aux $(N-N\times p)/2$
\end{itemize}  
\figScaleX{0.6}{stat_permutations}{\textit{"One-tailed"} et \textit{"two-tailed"} test}
Grâce à cette méthode d'évaluation statistique, nous pourrons par exemple conclure si l'on a une augmentation, une diminution ou une différence statistique entre une valeur de puissance et la puissance contenue dans une période de baseline. Dernière précision, on comprend ainsi que pour obtenir une valeur $p$ il faut que la taille de la distribution $N$ soit au moins de $1/p$.



% ********************************************
%                 PUISSANCE
% ********************************************
\subsection{Puissance spectrale}

\subsubsection{Méthodes explorées}
Le calcul de la puissance spectrale a été approché par deux méthodologies et qui ont été utilisés à des fins différentes :
\begin{itemize}
	\item La transformée d'Hilbert: souvent exploité dans le cadre du décodage ainsi que pour garder une uniformité entre les attributs de phase et \pacFR basés eux aussi sur cette transformée.
	\item La transformée en ondelettes: principalement utilisée pour la visualisation des cartes \tf à cause de l'adaptation des ondelettes aux bandes physiologiques.
\end{itemize}

\subsubsection{Normalisation}
On utilise la normalisation pour observer l'émergence d'un phénomène par rapport à une période définie comme baseline. A travers la littérature, quatre grands types de normalisation sont rencontrés:
\begin{enumerate}
	\item Soustraction par la moyenne de la baseline
	\item Division par la moyenne de la baseline
	\item Soustraction puis division par la moyenne de la baseline
	\item Z-score: soustraction de la moyenne puis division par la déviation de la baseline
\end{enumerate}
La normalisation z-score est certainement la plus fréquemment rencontrée à travers la littérature. Le choix du type de normalisation dépend du type de données utilisées. Dans le cadre de nos données, \textit{3.} était clairement la plus adaptée pour la visualisation. En revanche, dans le cadre de la classification, nous obtenions systématiquement de meilleurs résultats sans normalisation.

\subsubsection{Évaluation statistique}
La fiabilité statistique de la puissance a été évaluée en comparant chaque valeur de puissance à la puissance contenue dans une période définie comme baseline. Pour ce faire, nous avons testé deux approches:
\begin{enumerate}
	\item Permutations : les valeurs de puissance et de baseline sont aléatoirement mélangées à travers les essais. Puis, on normalise cette puissance. En répétant cet procédure $N$ fois, on obtient une distribution qui peut ensuite être utilisée pour en déduire la valeur $p$ de la véritable puissance (cf: \textit{pré-requis})
	\item "Wilcoxon signed-rank test": ordonne les distances entre les paires de puissances (vraie valeur, baseline) \citep{demandt_reaching_2012, rickert_encoding_2005, waldert_hand_2008}
\end{enumerate}
\figScaleX{0.6}{ossandon_tf}{Exemple de représentation temps-fréquence de puissance normalisées z-score \citep{ossandon_transient_2011}}



% ********************************************
%                     PHASE
% ********************************************
\subsection{Phase}
L'extraction de la phase se fait de la m\^{e}me manière que pour le \PACFR, en prenant l'angle de la transformée d'Hilbert d'un signal filtré. La significativité peut être évaluée en utilisant le test de Rayleigh \citep{jervis_fundamental_1983, tallon-baudry_oscillatory_1997}. Point de vue pratique, cela correspond à la fonction \textit{circ\_rtest} de la toolbox Matlab \textit{CircStat} \citep{berens_circstat_2009}




% ********************************************
%                   PAC
% ********************************************
\subsection{\PACEN}
Le calcul du \PACEN ne se limite pas uniquement à la méthode. En réalité, pour obtenir une estimation fiable sur des données réelles, il est indispensable de suivre les trois étapes suivantes:
\begin{enumerate}
	\item Estimation de la véritable valeur de PAC. Il existe plusieurs méthodes.
	\item Calcul de "\textit{surrogates}": on va calculer des PAC déstructurés. Idem, il existe de nombreuses méthodes
	\item Correction du véritable PAC par les "\textit{surrogates}". Cette correction, qui est en faite une normalisation, aura pour but de soustraire à l'estimation du PAC de l'information considérée comme bruitée.
\end{enumerate}
Les sous-parties suivantes présenteront de manières succinctes les principales méthodes rencontrées dans la littérature, ainsi que différents types de corrections applicables.

\subsubsection{Méthodologie du \pacEN}
Il existe une large variété de méthodes pour calculer le PAC, ce qui complique son exploration. Toutefois, il n'existe pas de consensus sur une méthode plus polyvalente qu'une autre, chacune possédant ses points forts et limitations.
Pour aller un peu plus loin, et présenter quelques méthodes, il est nécessaire d'introduire quelques variable. Soit $x(t)$, une série temporelle de données de taille N. Pour cette série temporelle, on souhaite savoir si la phase extraite dans une bande de fréquence $f_{\phi}=[f_{\phi_{1}},f_{\phi_{2}}]$ est couplée avec l'amplitude contenue dans $f_{A}=[f_{A_{1}},f_{A_{2}}]$. Pour cela, on va tout d'abord extraire $x_{\phi}(t)$ et $x_{A}(t)$ les signaux filtrés dans ces deux bandes. Enfin, la phase $\phi(t)$ est obtenue en prenant l'angle de la transformée d'Hilbert de $x_{\phi}(t)$ tandis que l'amplitude $a(t)$ est obtenue en prenant le module de la transformée d'Hilbert de $x_{A}(t)$. 

\begin{enumerate}

	% MEAN VECTOR LENGTH
	\item \mvl: \\
	Cette méthode à été introduite par \cite{canolty_high_2006} et consiste à sommer, à travers le temps, le complexe formé de l'amplitude des hautes fréquences avec la phase des basses fréquences. L'équation est donnée par:
	\begin{equation}
	MVL = |\sum_{j=1}^{N} a_(j) \times e^{j\phi(j)}|
	\end{equation} \\
	
	% KULLBACK-LEIBLER DIVERGENCE
	\item \kld:\\
	A l'origine, la divergence de Kullback-Leibler (KLD), qui est issue de la théorie de l'information, permet de mesurer les dissimilarités entre deux distributions de probabilités. Ainsi, pour pouvoir utiliser cette mesure dans le cadre du PAC, \cite{tort_measuring_2010} propose une solution élégante qui consiste à générer une distribution de densité probabilités de l'amplitude (DPA) en fonction des valeurs de phase et d'ensuite utiliser le KLD pour comparer cette distribution à la densité de probabilité d'une distribution uniforme (DPU). Plus la DPA s'éloigne de la DPU, plus le couplage entre l'amplitude et la phase est consistant. \\
	Pour construire la DPA, l'astuce consiste à couper le cercle trigonométrique en N tranches (dans l'article il est proposé de couper en 18 tranches de 20°). Puis, si on prend l'exemple de la tranche $[0,20°]$, on va chercher tout les instants temporels où la phase prend des valeurs comprises entre $[0,20°]$ ($t, \phi(t) \subset [0,20°]$). On prend ensuite la moyenne de l'amplitude pour ces valeurs de $t$ et on répète cette procédure pour chacune des tranches de phase. On obtient ainsi la densité d'amplitudes en fonction des valeurs de phase. Il ne reste plus qu'à normaliser cette distribution par la somme des amplitudes à travers les tranches et on récupère une distribution de densité de probabilités. La figure \ref{fig:PAC_plot_Tort_2010} \citep{tort_measuring_2010} présente un exemple de DPA en fonction de tranches de phase. \\

	\figScaleX{0.6}{PAC_plot_Tort_2010}{Densité de probabilité d'une distribution d'amplitudes en fonction de tranches de phases}
	
	Le calcul de la divergence de Kullback-Leibler est ensuite appliqué pour mesurer les dissimilarités entre la DPA et la DPU et c'est cette mesure qui servira d'estimation du \pacFR:
	\begin{equation}
	D_{KL}(P, Q) = \displaystyle\sum_{j=1}^{N} P(j) \times \log{\frac{P(j)}{Q(j)}}
	\end{equation} \\
	où $P(j)$ est la densité de probabilité de $a(t)$ en fonction de $\phi(t)$ et $Q(j)$ est la densité de probabilité d'une distribution uniforme. \\
	
	% HEIGHT-RATIO
	\item \hr \\
	La méthode du \hr \citep{lakatos_oscillatory_2005} est extr\^{e}mement proche du \kld. En effet, l'amplitude sera binée de la même façon en fonction des tranches de phase. La mesure du PAC est ensuite donnée par:
	\begin{equation}
	hr=(f_{max}-f_{min})/f_{max}
	\end{equation} \\
	où $f_{max}$ et $f_{min}$ sont respectivement le maximum et le minimum de la de la densité de probabilité de l'amplitude en fonction des valeurs de phase. \\
	
	% NDPAC
	\item \ndpac \\
	Le \ndpac, qui n'est pas une des méthodes les plus fréquemment rencontrées, présente toutefois une avantage certain. En plus de fournir une estimation fiable du \pacFR, \cite{ozkurt_statistically_2012} démontre l'existence d'un seuil à partir duquel on peut considérer l'estimation du PAC comme étant statistiquement fiable. La beauté de cette méthode, c'est que ce seuil statistique, qui est une fonction de la valeur p désirée, ne dépend que de la taille de la série temporelle. Ce qui rend son utilisation particulièrement simple. \\
	Pour estimer le PAC, une des hypothèses ayant permis d'aboutir à ce seuil statistique est de devoir normaliser l'amplitude par un z-score dénotée $\tilde{a}(t)$. L'estimation du PAC est quasiment identique au MVL puisque c'est en réalité le carré de celle-ci. Enfin, pour une valeur p désirée, l'article introduit le seuil statistique: \\
	\begin{equation}
	x_{lim}=N \times [erf^{-1}(1-p)]^{2}
	\end{equation} \\
	
	où $erf^{-1}$ est la fonction d'erreur inverse. On déduira que l'estimation PAC est significative si et seulement si cette valeur est deux fois supérieur à ce seuil. \\
	
	% AUTRES
	\item Autres méthodes:
	Tout les algorithmes présentés ci-dessus ont été testés, implémentés et comparés. En complément, voici une liste non exhaustive d'autres méthodes existantes:
	\begin{itemize}
		\item \textit{Phase Locking Value (PLV)} \citep{cohen_assessing_2008, penny_testing_2008}: détournement du PLV proposé par \cite{lachaux_measuring_1999} qui mesure la synchronie de phase entre deux électrodes. Cette méthode va comparer la phase des basses fréquences avec la phase de l'amplitude des hautes-fréquences.
		\item \textit{Generalized Linear Model (GLM)} \citep{penny_testing_2008}: outil décrit comme adapté aux données courtes et bruitées.
		\item \textit{Generalized Morse Wavelets (GMW)} \citep{nakhnikian_novel_2016}: basée sur des ondelettes, semble particulièrement utile dans le cadre de l'exploration des données.
		\item \textit{Oscillatory Triggered Coupling (OTC)} \citep{dvorak_toward_2014, watrous_phase-amplitude_2015}: issue d'une détection de maximums des hautes fréquences.
	\end{itemize}

\end{enumerate}

% PERMUTATIONS ET NORMALISATION
\subsubsection{Correction du \pacEN et évaluation statistique}
Nous avons vu dans la section précédente différentes méthodes permettant de calculer un \PACFR. Toutefois, celui-ci peut être largement améliorée en faisant une estimation du PAC contenu dans le bruit des données. Une fois que cette estimation sera faite, on pourra retrancher ce PAC bruité à la valeur initiale. Tout comme il existe plusieurs méthodes de PAC, les équipes de recherche proposent à tour de rôle de nouvelles méthodes. Parmi elles, on peut citer:
\begin{itemize}
	\item \textit{Time-lag}: proposée par \cite{canolty_high_2006}, on introduit un délai sur l'amplitude compris entre $[f_{s},N-f_{s}]$ où $f_{s}$ est la fréquence d'échantillonnage et $N$ est le nombre de points de la série temporelle
	\item \textit{Shuffling des couples [phase,amplitude]}: ici, on mélange aléatoirement les essais de phase et d'amplitude \citep{tort_measuring_2010}
	\item \textit{Swapping temporel d'amplitudes (ou de phase)}: on mélange aléatoirement les essais d'amplitude puis on recalcule le PAC avec la phase originale \citep{bahramisharif_propagating_2013, lachaux_measuring_1999, penny_testing_2008, yanagisawa_regulation_2012} 
\end{itemize}
Ces trois méthodes produisent une distribution de \textit{surrogates}. On pourra ensuite appliquer un z-score à la véritable estimation en utilisant la moyenne et la déviation de cette distribution. Enfin, l'évaluation statistique se fait également à partir de cette distribution  (cf: \textit{pré-requis}) \\
A ma connaissance, il n'existe pas de comparatif entre ces corrections et je n'ai jamais rencontré d'articles mentionnant que l'on ne puisse pas combiner les méthodes de PAC avec les différentes corrections. En revanche, ce qui est relaté c'est que le \textit{time-lag} nécessite des données longues d\^{u} à l'introduction de ce délai temporel. 

% COMPARATIF
\subsubsection{Comparatif des méthodes}
\cite{penny_testing_2008} ont comparé plusieurs méthodes dont le \textit{MVL}, \textit{PLV} et le \textit{GLM} et \cite{tort_measuring_2010} ont complété cette étude avec d'autres méthodes comme le montre le tableau ci-dessous. Enfin, \cite{canolty_functional_2010} a fait une review qui comprend un descriptif très instructif.
\begin{figure}[H]
	\hspace*{-1in}
	\includegraphics[scale=0.35]{./figures/PAC_methods_comparison}
	\caption{Comparatif de méthodes d'évaluation de \pacFR}
\end{figure}

% VISUALISATION
\subsubsection{Représentation du \pacEN}
Comparée à la puissance, l'exploration du PAC peut s'avérer plus complexe d\^{u} à sa dimensionnalité plus grande. Il existe donc des outils et des méthodes destinées à simplifier cette exploration et à visualiser ces résultats. \\
Exemple concret, si on cherche à conna\^{i}tre les modulations de puissance contenue dans un signal, on peut représenter une carte \tf. Pour le PAC, idéalement on voudrait visualiser les phases, les amplitudes et le temps mais ces trois dimensions empêche une représentation simple. On peut donc avoir recours à différents types de représentations complémentaires:
\begin{itemize}
	\item Puissance phase-locked : cette représentation permet de faire émerger l'existence d'un couplage, pour une phase donnée, et d'observer sa durée. Pour cela, on aligne les phase en détectant le pic le plus proche de l'instant temporel étudié. On calcul les cartes \tf que l'on va ensuite moyenner après les avoir recalées de la même façon que les phases (\cad avec la même latence).
	\item Comodulogramme : pour une tranche temporelle définie, on représente les valeur de PAC pour différentes valeurs de phase et d'amplitude
\end{itemize}

\figScaleX{1}{hemptinne_2013}{\textbf{(A)} Exemple de cartes temps-fréquence phase locked sur le $\beta$, \textbf{(B)} Exemple de comodulogramme}

La figure \ref{fig:hemptinne_2013} \citep{hemptinne_exaggerated_2013} met en évidence que la représentation des cartes temps-fréquence phase-locked \textbf{(A)} est limitée d'une part, par la phase sur laquelle on choisit de recaler et d'autre part cette méthode est également limité par l'instant où l'on choisit de recaler. Pour la figure \textbf{(B)}, le calcul du PAC se faisant à travers la dimension temporelle, on a aucune idée de l'évolution du couplage dans le temps. \\

% ERPAC
\subsubsection{\PACEN: résolution temporel?}
Comment peut-on savoir si un ensemble de musiciens jouent ensemble, en rythme? L'approche traditionnelle consiste à dire que, en fonction de la prestation du groupe, on sera en mesure de dire si ils étaient en rythme ou non. Donc on focalise notre attention sur chaque instant du morceau et on analyse chaque note, chaque décalage. Cela signifie aussi que toute notre attention a été mobilisée par l'analyse du rythme et finalement, on passe à côté de la musique. Notre attention au détail nous a écarté du morceau global. On pourrait dire que l'on a écrasé la dimension temporelle du morceau. Une autre approche consiste à assister à toute les répétitions du fameux groupe. Ce faisant, on est capable de dire si d'une manière générale les musiciens ont tendance à jouer ensemble. Ainsi, le jour d'une représentation, toute notre attention peut rester uniquement sur le concert. On garde donc la dimension temporelle.\\ 
C'est par ce changement de positionnement face au problème de résolution temporelle que \cite{voytek_method_2013} introduit le \erpac . L'approche traditionnelle du PAC nécessitant de connaître un nombre de cycles afin d'en déduire l'existence ou non du couplage, et donc perdre la dimension temps, l'article propose de calculer le PAC à travers les essais (ou répétitions). Pour un jeu de données de M essais de longueur N, on extrait respectivement les phases et les amplitudes $\phi_{M}(t)$ et $a_{M}(t)$ puis, pour chaque point temporel, on calcul la corrélation à travers les essais (corrélation linéaire-circulaire \citep{berens_circstat_2009} qui se fait entre l'amplitude et des sinus/cosinus de la phase). Il en résulte une valeur de corrélation pour chaque instant et donc, de couplage. \\



\section{Apprentissage supervisé}
% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
%                           APPRENTISSAGE SUPERVISE
% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
Présentation du concept
Training set et Testing set

\subsection{Labellisation et apprentissage}

\subsection{Classifieurs}
\begin{enumerate}

	% LINEAR DISCRIMINANT ANALYSIS
	\item \lda \\
	
	\citep{fisher_use_1936}, \citep{lotte_review_2007} \\
	
	
	\figScaleX{0.4}{LDA_Lotte_2007}{Principe du \lda}
	
	% SUPPORT VECTOR MACHINE
	\item \svm \\
	\citep{cortes_support-vector_1995, vapnik_nature_2000}
	\citep{lotte_review_2007}
	\figScaleX{0.4}{SVM_Lotte_2007}{Principe du \svm}
	
	% K-NEAREST NEIGHBOR
	\item \knn
	
	% NAIVE BAYES
	\item \nb
	
	% RANDOM FOREST
	\item \rf
\end{enumerate}

\begin{figure}[H]
	\hspace*{-0.7in}
	\includegraphics[scale=0.35]{./figures/scikit_classifier}
	\caption{Comparatif de classifieurs (scikit-learn)}
\end{figure}

\subsection{Cross-validation}
Présentation et utilisation (contexte: séparation Training et Testing set // optimisation des paramètres (classifieurs et \mf))

\begin{enumerate}
	\item \kfold
	
	\kfold , \skfold , \shsp, et \sshsp
	\item \loo

\end{enumerate}
%\verb+\+figScaleX\{0.6\}\{LDA\_Lotte\_2007\}\{Principe du \verb+\+lda\}



\subsubsection{Évaluation de la performance de décodage}
\begin{enumerate}
	\item \da
	\item \roc
\end{enumerate}

\subsubsection{Évaluation statistique de la performance de décodage}
\begin{enumerate}
\item Loie binomiale
\item Permutation: data driven + différents types de permutations \citep{ojala_permutation_2010}
\begin{itemize}
\item Shuffle y
\item Full-shuffle
\item Intra-class shuffle y
\end{itemize}
\end{enumerate}

\subsection{Sur-apprentissage}

\subsubsection{Optimisation des paramètres de classification}
% -----------------------------------------------------------------------------
% -> Single et Multi-features
\section{Du single au multi-features}
Présentation du concept
\subsection{Single-feature}

\subsection{Multi-features}

\begin{enumerate}

\item Sélection statistique
\begin{enumerate}
\item Sélection binomiale
\item sélection permutations
\end{enumerate}
\item Sélection séquentielle
\begin{enumerate}
\item Forward selection
\item Backward selection
\item exhaustive selection
\end{enumerate}

\end{enumerate}